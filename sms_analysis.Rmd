Analyzing relationship qualities through short message service breakdowns and sentiment analysis. <--!more--> 

A good friend of mine consistently suggests new sources of data for potentially interesting narratives, so I wasn't quite surprised when he conferred with Dr. Google and dug up a python script that extracts messages from iPhones. This script, by [Tom Offerman](https://github.com/toffer/iphone-sms-backup), deposits a copy of all your SMS and iPhone history in an SQLite database. From there, it's an easy export to csv or json, and a straight dump into R, where one can run all sorts of analyses. 

What can we get from a dataset like this? Think about what happens during an iMessage transaction. You receive or send a message from or to someone at a particular time during the day. The message is usually plain text (with the exception of emoticons) and usually rooted in English. Essentially, a transaction involves the message sent, the person who sent it, the person who received it, and a timestamp. Nice and simple.

From this body of data, one can surmise several questions. With cumulative SMS data, one can probably predict some sort of trend, keeping in mind that constructing a narrative is infinitely more interesting when we are able to relate and reflect upon the data itself. Nobody *really* thinks about general SMS trends (perhaps SMS developers?); analyzing personal SMS data only becomes cool and engaging when there's some skin in the game. What I mean is: have I been communicating (or not communicating) with my significant other, friends, and family? Has that communication been negative or positive? It's these (sometimes) painful epiphanies from the data that incites introspection, which hopefully leads to behavior change. 

Let's explore the data. The exported iPhone data is rather simple. It shows you a date/time, a "to" number, a "from" number, and the text message contents. "Me" has been generously labeled for one's own number in the "to" and "from" columns.

```{r}

```

If we aggregrate and calculate some statistics for this data, we can derive some interesting insights. Because we have all the texts that have been sent and received and all the text contents for each subsequent transaction, we're able to calculate:

1. the average response rate for each person. (sending/receiving text number)
2. the average words per text message. (total bag of words/total texts)

The point of this is to get a general picture between two individuals; who sends and recieves more messages, and the length of such messages. 

So, in order to continue, I matched the phone numbers with the names of the people that I associate them with. This is for ease of identification. In order to get some insight on the first question, one has to create a table detailing the response rate; this essentially is a table that outlines the number of texts that a person has sent and received from me. I won't bore you with the details, but I utilized `dplyr`'s select and filter function to come up with something like this:

```{r}

```

I filtered the table so that only those who have greater than 300 total messages appear. I'm going to focus on two examples, the contact named "Beans" (interesting story behind the nickname) and "Joyce". What we're interested in here is the column named `respRatio`. This is the response rate, the number of texts I send as compared to the number of texts that the other person sends. Most of them are above 1, which indicates that I'm fairly good at responding to them (on average, I send them a text for every text they send me). It gets a bit worrisome with Joyce; for every text she sends, I, on average, only respond 75% of the time. One may conclude that there might be some rough patches in my relationship to Joyce; but there's not. Joyce is my sister, and we don't usually communicate by text. I can recall several times where her text didn't warrant an immediate response, if at all. But you can see where I'm going with this type of information.

Based on total texts alone, its fairly obvious I text Beans fairly often. It's not a bad guess to say that Beans has significance to me, since the total number of texts outnumber the others by several factors. We might be making an assumption that large numbers of texts means "important", but I can't think of any other reason why I would text someone who wasn't/isn't important. There's no denying that Beans holds significance to me- she's my lovely girlfriend. The response rate is a hearty 1.06, which indicates that the conversation isn't one sided (nice!).

Using this text summary we can derive further insight on average words per text, and for kicks, average profanities per text. To do this for all transactions between me and Beans, we need to get the total number of words or profanities exchanged between the two of us, and divide that by the total number of texts.

Getting a count of the words is rather easy. Imagine I take every word in every text I send to Beans and every word she sends to me and throw it onto a page, with no punctuations, all stop words removed (words like "the", "a", "an", "which", etc) and a space between each word. This is essentially, a "bag of words." Counting them by hand would suck, but luckily we have R.

Procuring a count of profanities is a bit trickier. This requires one to understand the "bag of words" concept. Using our bag of words we would provide an additional step of filtering for all the profanities in a bag of words. Let's break it down. In the case with Beans, I specifically looked for "fuck" "shit" "damn" "bitch" and "ass." The function below is what I previously constructed, but it was not specific enough. Let's go through it. 

```{r, eval=FALSE}
profanityWords <- function(user) {
	swearWords <- "fuck|shit|damn|bitch|ass"
	profanities <- exchangedWords(user)[grep(swearWords, 
											 exchangedWords(user))]
	return(profanities)
}
```

1. I created a function to return a vector of profanity words.
2. Within the function, I created a variable that included 5 main swear words, separated by the "`|`" operand, which stands for "or."
3. The variable `profanities` returns a subset of words that fit the regular expression `swearWords`, which returns a list of words that contain those five words. 

Notice I said "contain" those five words. We do indeed get the words we wanted. For example, we got "fuck", "shit", "bitch", etc. But we also got "shitstorm", "bullshit", and "goddamn"- this is because regular expressions works in that it finds all standalone instances of the word and any word that includes the word. Because this is the case, we also get problematic words such as "glass", "class", "embarrass", "kickass", which technically are not considered swear words. So I modified the function below: 

```{r, eval=FALSE}
profanityWords <- function(user) {
	swearWords <- "fuck|shit|damn|bitch|^ass$|^(ass)([chlw]{1}).*"
	profanities <- exchangedWords(user)[grep(swearWords, 
											 exchangedWords(user))]
	return(profanities)
}
```

Because "ass" had a tendency to show up in alot of other words (assume, class, glass, assemble), I changed the regular expression to match only ass, and only words that followed ass with the letters "c, h, l, or w." "`^ass$`" searches for instances of ass by itself; the `^` indicates "at the beginning" and the `$` indicates "at the end." You can probably guess the next regular expression. `^(ass)` finds ass at the beginning (starts with) and then a c, h, l, or w that occurs once after ass. This picks up the most common variations of ass (asscracks, assholes, assloads, and asswipes). [This](http://regexpal.com/) is a rather fun site to go and get your regexp on. 

So after all of this `grep`ping, I was able to find out that between me and Beans, our average word length for a text was 4.65 words, and the average profanities per text was approximately 0.02. That seems reasonable, given that a sentence is approximately 10 words, and some texts can be as short as "K;" I'd expect the average to be around 5. The rather low appearance of profanities most likely indicates a good thing. Neither of us are potty mouths/texters (unless I'm looking for parking (subsequent rage-fest), and usually it's not recorded because I'm not on my phone). 

---

The next part gets interesting. Remember that bag of words? We're going to conduct sentiment analysis on it, and it's going to spit back a score that rates our overall sentiment as concluded by the cumulative positivity or negativity of the bag of words. 

The concept is fairly simple. We take every word that Beans and I have exchanged, and give each word a score of $+x$ or $-x$, where $x$ is a number indicating how positive or how negative the word is. There are pre-made dictionaries that can be downloaded, which include standard words that have manually been scored. An example of several words is provided below. All that there is left to do is the match the words in the Beans bag with the pre-scored words from the sentiment dictionary. The respective scores in the Beans bag are all added up, and divided by the total number of words. This gives us the mean sentiment score for all texts between me and Beans.

```{r}

```

So my mean sentiment score with Beans is -40, which tells me that of all of our texts so far, the average sentiment comes out to be slightly negative. What is this indicative of? Technically, nothing about causation can be said, but it does reveal that perhaps more positive topics and outlook couldn't hurt. In my case, I've ended up on the wrong side with Beans. Time to switch up the language. However, note this is the absolute mean. It may be better to look at average sentiment by month and by week, over time.

```{r}

```

We know that texts, for the most part, are neutral ("on my way!", "ok", "I'll see you there"). It seems normal that there are fluctuations up and down from 0. Graphing the sentiment score of every text I've ever received and ever texted shows a rather indistinguishable pattern.

```{r}

```

Same with each individual text with Beans. 

```{r}

```

However, when going by month, we see that when aggregated by month, the neutrality of texts remains (with the mean hovering close to 0), but there is just the slightest tendency to be positive; with a rather large spike and drop somewhere in January to April 2014. What could that be?

```{r}

```

So I decided to zoom in by week for those four months, to determine just what that spike was. 

```{r}

```

Looks like the highest point was in February, and the lowest point at the end of March. Let's checkout the content of those texts.

```{r}

```

In February, looks like there were various positive texts, including the words "love," "warm," "morning," "darling," and "interesting," which generated a fairly positive sentiment. I remember this particular walk with my friend Maggie; I was enjoying myself, and relaying the news updates to Beans. 

Taking a look at March reveals a particularly hilarious insight. It also points out a drawback of sentiment analysis. My lovely girlfriend is in veterinary school and was applying to internships at a major "provider" (let's say) of internships. There was a rude altercation between the hiring manager and my girlfriend (a real case of unprofessionalism), which is reflected in the strongly negative emotion of "cunt," as texted to me. I guess that slipped by the profanity filter (oops). Either way, that completely negated out the average for the week, all the way down to 0. Notice that Beans said "and I don't like that word," but still received a sentiment score of +1. This is one the drawbacks of sentiment analysis, where the simplest models do not take negation into account. 

Additional drawbacks include the fact that sentiment dictionaries are limited by a set number of words. Previously made cuss word filters may not catch emoticons, and in general, emoticons needed to be treated entirely differently, as their ASCII form usually comes out to a bunch of garbled punctuation, which is filtered out during the corpus creation. SMSes also contain irregular and user created language and abbreviations which are considered misspellings and will obviously end up as gaps within the sentiment picture. Sentiment analyses like this one are also fairly literal, so it doesn't take into account intrinsic emotions, sarcasm, and rhetoric. It also doesn't account for lost messages (the time my iPhone decided to take a swim in a lake).

More on sentiment to come! Perhaps a Shiny application...

---

first calculate sentiment for each individual text using a loop
check out distribution of sentiment for Beans only
take note of highs and lows
go by average sentiment per week per year
zoom in on drop year, month, and week
check out the actual text

# measurable metrics
* sentiment score per person (normalized by bag of words)
* average cuss words per text(find fuck shit bitch damn ass butt suck / total texts
* sentiment score by text
* sentiment score over time (group by bin posixct year/month/01)
	* check out actual language for highest and lowest sentiment score texts
	* less granulated sentiment score over time
	* the day/period with the widest range of sentiment scores (moodiest day?)
	* correlate negative sentiment days with cuss words on which day

# drawbacks
* limited dictionaries
* cuss words and filters may not take into account emoticons
* cuss words and filters don't take into account misspellings
* doesn't take into account intrinsic emotions, sarcasm
* doesn't take into account self-created language
* doesn't account for time-based emotion
* doesn't account for lost messages (iPhone thrown into the lake)
* doesn't take into negation

# shiny application

---

```{r}
### functions ################################################################
numTextsReceived <- function(yourName) {
	sms %>% group_by(fromName, toName) %>%
	summarize(count=n()) %>%
	filter(toName==yourName) %>%
	rename(key=fromName, textsToMe=count)
}

numTextsSent <- function(yourName) {
	sms %>% group_by(fromName, toName) %>%
	summarize(count=n()) %>%
	filter(fromName==yourName) %>%
	rename(key=toName, textsToUser=count)
}

textSummary <- function() {
	merge(numTextsReceived("Me"), numTextsSent("Me"), by="key") %>%
	select(-toName, -fromName) %>%
	mutate(total=textsToMe+textsToUser, respRatio=textsToUser/textsToMe) %>%
	filter(total>300) %>%
	arrange(desc(respRatio))
}

wordsPerText <- function(user, words) {  # calculates words per text
	words <- length(words)
	texts <- textSummary() %>% 
		filter(key==user) %>% 
		select(total) %>%
		rename(wordsPerText=total)
	return(words/texts)
}

exchangedWords <- function(user) {  # calculates total words exchanged
	userBag <- createBag(user) %>% 
		strsplit(, split=" ") %>% 
		unlist() %>% 
		.[-grep("^$", .)]
	return(userBag)
}

profanityWords <- function(user) {
	swearWords <- "fuck|shit|damn|bitch|^ass$|^(ass)([chlw]{1}).*"
	profanities <- exchangedWords(user)[grep(swearWords, 
											 exchangedWords(user))]
	return(profanities)
}

### sentiment scores #########################################################

sentimentScore <- function(user) {
	score <- user %>% createBag %>% bagToDict %>% sentimentMean
	message(sprintf("Your sentiment score with %s is %s!", user, score))
}

sentimentMean <- function(userDict) {
    matchWordsIndex <- na.omit(match(userDict[, 1], sentDict[, 1]))
    sentWords <- sentDict[matchWordsIndex, 1]
    sentScore <- sentDict[matchWordsIndex, 2]
    sentFreq <- userDict[match(sentWords, userDict[, 1]), 2]
    return(sum(sentScore*sentFreq)/length(userDict))
} 

### bag of words #############################################################

bagToDict <- function(userBag) {  # remove empty elements, sort
    vBag <- userBag %>% as.character %>% 
    	strsplit(., split=" ") %>% unlist %>% unique %>% sort
	userBagTab <- vBag %>% factor %>% tabulate 
    userDict <- data.frame(word=vBag, count=userBagTab) %>%
    	arrange(., desc(count))
    return(userDict)
}

createBag <- function(user) {  # creates bag of words by user
	userVec <- sms %>%
		filter(toName==user | fromName==user) %>%
		select(text) %>%
		unlist(, use.names=FALSE) %>%
		as.vector() %>%
		processBag()
	userBag <- paste(userVec, collapse=" ")
	return(userBag)
}

processBag <- function(vec) {  # create corpus object, extract content
	v <- VectorSource(vec) %>% 
		Corpus %>%
		tm_map(., content_transformer(tolower)) %>%
		tm_map(., removePunctuation) %>%
		tm_map(., stemDocument) %>%
		tm_map(., stripWhitespace)
	vc <- vector()
	for (i in seq(length(v))) { vc[i] <- v[[i]]$content } 
	for (i in seq(length(vc))) { if (vc[i]=="") { vc[i] <- "the"} }
	return(vc)
}

### load sentiment dictionary ################################################

processSentDict <- function(dict) {  # add names and convert to numeric
    names(dict) <- c("word", "score")
    dict$score %<>% as.numeric
    return(dict)
}

loadSentDict <- function(filepath) {  # read in sentiment dictionary
    con <- file(filepath)
    fileLines <- readLines(con)
    close(con)
    dict <- strsplit(fileLines, split="\t") %>% 
    	do.call(rbind, .) %>% 
    	as.data.frame(., stringsAsFactors=FALSE)
    return(dict)
}

### filter by user ###########################################################

smsPerson <- function(person) {
	return(sms %>% filter(fromName==person | toName==person))
}

```

```{r}
library("dplyr")
library("magrittr")
library("ggplot2")
library("tm")
library("tidyr")
setwd("~/Copy/datasets/iphone")
sms <- read.csv("sms_data.csv")
```

```{r}
### match numbers with contact names #########################################
contactMatch <- c("Rock", "Me", "Joon", "Andrew", "Beans", "Mom", "Niels", 
				  "Margaret", "Quentin", "Joyce", "Michael", "Whitney", 
				  "Quanster", "Ingi", "Tsai", "Wendell", "Katherine", 
				  "Miranda", "Jack", "Jen", "Jacob", "Ferguson", "Unknown",
				  "Jay", "Umar", "Anita", "Chang", "Brian", "Lanza", "Tina", 
				  "Ni", "Tsai", "Wes", "Spring", "Shishir", "Tanya")

contactList <- sms %>% 
	select(To) %>% 
	unique() %>%
	rename(Number=To) %>%
	mutate(Name=contactMatch)

sms %<>%
	separate(Date, c("date", "time"), sep=" ", remove=T) %>%
	transmute(date=as.POSIXct(date, tz="GMT", "%Y-%m-%d"), 
			  year=strftime(date, format="%Y"),
			  month=strftime(date, format="%B"),
			  week=strftime(date, format="%W"),
			  #time=time, as.POSIXlt(time, tz="GMT", "%H:%M:%S"), 
			  fromName=contactList[match(From, contactList[, 1]), 2],
			  toName=contactList[match(To, contactList[, 1]), 2], 
			  text=Text)

### sentiment dictionary call ################################################

sentDict <- loadSentDict("AFINN-111.txt") %>% processSentDict()

```

```{r}
textSummary()
wordsPerText("Beans", exchangedWords("Beans"))
wordsPerText("Beans", profanityWords("Beans"))
sentimentScore("Beans")
```

```{r}
### sentiment scores for each text ###########################################

sms %<>%
	mutate(sentScore=sapply(text, function(x) x %>%
		   processBag %>%
		   bagToDict %>%
		   sentimentMean))

### plot avg sentiment score for each and every transaction ##################

ggplot(sms, aes(date, sentScore)) + 
	geom_point() +
	ggtitle("Text Sentiment by Time") +
	xlab("Date") + 
	ylab("Sentiment")

highEmotions <- sms %>% 
	filter(sentScore>=5 | sentScore<=-5) %>%
	select(text, sentScore)
```

```{r}
### avg sentiment score by month/week based on individual avgs ###############
smsBeans <- smsPerson("Beans")

smsBeansMonth <- smsBeans %>% 
	group_by(year, month) %>%
	summarize(avgScore=mean(sentScore)) %>%
	transmute(ymd=as.Date(paste(year, month, "01", sep="-"), 
						 format="%Y-%B-%d"),
			  avgScore=avgScore)

smsBeansJanApr <- smsBeans %>%
	group_by(year, month, week) %>%
	summarize(avgScore=mean(sentScore)) %>%
	filter(year==2014 & 
		   month %in% c("January", "February", "March", "April")) %>%
	mutate(week=seq(from=1, to=28, by=6)) %>%
	transmute(ymw=as.Date(paste(year, month, week, sep="-"),
						  format="%Y-%B-%d"),
			  avgScore=avgScore)

ggplot(smsBeans, aes(date, sentScore)) + 
	geom_point() +
	ggtitle("Individual Text Sentiment by Time") +
	xlab("Date") + 
	ylab("Sentiment")

ggplot(smsBeansMonth, aes(ymd, avgScore)) +
	geom_point() +
	geom_line() +
	ggtitle("Text Sentiment by Month (Jul 2013-Jan 2015)") +
	xlab("Date") + 
	ylab("Average Sentiment Per Text") +
	theme_bw()

ggplot(smsBeansJanApr, aes(ymw, avgScore)) +
	geom_point() +
	geom_line() +
	ggtitle("Text Sentiment by Week (Jan-Apr 2014)") +
	xlab("Date") + 
	ylab("Average Sentiment Per Text") +
	theme_bw()
```

```{r}
outliersFebBeans <- smsBeans %>%
	filter(year==2014 & 
		   month=="February" &
		   week=="04")

outliersMarBeans <- smsBeans %>%
	filter(year==2014 & 
		   month=="March" &
		   week=="13")
```








